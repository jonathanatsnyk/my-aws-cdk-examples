{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb80997a-f72d-4dfd-bbe3-635bee9f0548",
   "metadata": {},
   "source": [
    "# Train Linear Learner model with MNIST using Amazon FSx for Lustre\n",
    "\n",
    "This notebook example is similar to [An Introduction to Linear Learner with MNIST](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/linear_learner_mnist/linear_learner_mnist.ipynb).\n",
    "\n",
    "[An Introduction to Linear Learner with MNIST](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/linear_learner_mnist/linear_learner_mnist.ipynb) has been adapted to walk you through on using AWS FSx for Lustre (FSxLustre) as an input datasource to training jobs.\n",
    "\n",
    "Please read the original notebook and try it out to gain an understanding of the ML use-case and how it is being solved. We will not delve into that here in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e36a3d4-b0a6-4367-8f81-6fba43686b30",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfafbab2-b632-44f9-a98a-a5d11dcb6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.image_uris import retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f9595-3ded-4706-a040-b00b32112d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "container = retrieve(\"linear-learner\", region)\n",
    "\n",
    "bucket = Session().default_bucket()\n",
    "prefix = \"sagemaker/DEMO-linear-mnist\"\n",
    "output_location = f\"s3://{bucket}/{prefix}/output\"\n",
    "\n",
    "print(f\"sagemaker execution role: {role}\")\n",
    "print(f\"training artifacts will be uploaded to: {output_location}\")\n",
    "print(f\"container: {container}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f4da3c-a22b-46ef-9fc7-a0d53798b321",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b1d33d-a9e3-45eb-be6d-4bd731c7dd6b",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "Next, we read the MNIST dataset [1] from an existing repository into memory, for preprocessing prior to training. It was downloaded from this [link](http://deeplearning.net/data/mnist/mnist.pkl.gz) and stored on the downloaded_data_bucket.\n",
    "\n",
    "> [1] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, November 1998."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f34930-69f6-4730-9cb9-76b02c26a7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S3 bucket where the original mnist data is downloaded and stored.\n",
    "downloaded_data_bucket = f\"sagemaker-sample-files\"\n",
    "downloaded_data_prefix = \"datasets/image/MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee31f0-556d-4f11-9e32-08fdc1712ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle, gzip, json\n",
    "\n",
    "# Load the dataset\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(downloaded_data_bucket, f\"{downloaded_data_prefix}/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "with gzip.open(\"mnist.pkl.gz\", \"rb\") as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49857455-df72-4615-9e42-e108e668ee24",
   "metadata": {},
   "source": [
    "### Data inspection\n",
    "\n",
    "Once the dataset is imported, it's typical as part of the machine learning process to inspect the data, understand the distributions, and determine what type(s) of preprocessing might be needed. You can perform those tasks right here in the notebook. As an example, let's go ahead and look at one of the digits that is part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc352f96-42e5-4b5a-8912-828f8e173998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (2, 10)\n",
    "\n",
    "\n",
    "def show_digit(img, caption=\"\", subplot=None):\n",
    "    if subplot is None:\n",
    "        _, (subplot) = plt.subplots(1, 1)\n",
    "    imgr = img.reshape((28, 28))\n",
    "    subplot.axis(\"off\")\n",
    "    subplot.imshow(imgr, cmap=\"gray\")\n",
    "    plt.title(caption)\n",
    "\n",
    "\n",
    "show_digit(train_set[0][30], f\"This is a {train_set[1][30]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec4a45-4970-4255-909f-a880f1aeb4e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data conversion\n",
    "\n",
    "Since algorithms have particular input and output requirements, converting the dataset is also part of the process that a data scientist goes through prior to initiating training. In this particular case, the Amazon SageMaker implementation of Linear Learner takes recordIO-wrapped protobuf, where the data we have today is a pickle-ized numpy array on disk.\n",
    "\n",
    "Most of the conversion effort is handled by the Amazon SageMaker Python SDK, imported as sagemaker below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243d842-3a84-4648-8f93-baaddd6659ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "train_set_vectors = np.array([t.tolist() for t in train_set[0]]).astype(\"float32\")\n",
    "train_set_labels = np.where(np.array([t.tolist() for t in train_set[1]]) == 0, 1, 0).astype(\n",
    "    \"float32\"\n",
    ")\n",
    "\n",
    "validation_set_vectors = np.array([t.tolist() for t in valid_set[0]]).astype(\"float32\")\n",
    "validation_set_labels = np.where(np.array([t.tolist() for t in valid_set[1]]) == 0, 1, 0).astype(\n",
    "    \"float32\"\n",
    ")\n",
    "\n",
    "train_set_buf = io.BytesIO()\n",
    "validation_set_buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(train_set_buf, train_set_vectors, train_set_labels)\n",
    "smac.write_numpy_to_dense_tensor(validation_set_buf, validation_set_vectors, validation_set_labels)\n",
    "\n",
    "train_set_buf.seek(0)\n",
    "validation_set_buf.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e59b49-6869-417c-aeb1-8e929fe08f1b",
   "metadata": {},
   "source": [
    "### Upload training data\n",
    "\n",
    "Now that we've created our recordIO-wrapped protobuf, we'll need to upload it to S3, so that Amazon SageMaker training can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b32335-1e2c-4aa1-8956-8759d82810fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "key = \"recordio-pb-data\"\n",
    "boto3.resource(\"s3\").Bucket(bucket).Object(os.path.join(prefix, \"train\", key)).upload_fileobj(\n",
    "    train_set_buf\n",
    ")\n",
    "boto3.resource(\"s3\").Bucket(bucket).Object(os.path.join(prefix, \"validation\", key)).upload_fileobj(\n",
    "    validation_set_buf\n",
    ")\n",
    "\n",
    "s3_train_data = f\"s3://{bucket}/{prefix}/train/{key}\"\n",
    "s3_validation_data = f\"s3://{bucket}/{prefix}/validation/{key}\"\n",
    "\n",
    "print(f\"uploaded training data location: {s3_train_data}\")\n",
    "print(f\"uploaded validation data location: {s3_validation_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a57ac-877c-4409-80db-7f29fae384b2",
   "metadata": {},
   "source": [
    "## Prepare File System Input\n",
    "\n",
    "we specify the details of file system as an input to your training job. Using file system as a data source eliminates the time your training job spends downloading data with data streamed directly from file system into your training algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261c1cc-7022-45ba-868e-fbcefada5d70",
   "metadata": {},
   "source": [
    "### WARNING\n",
    "\n",
    "Before specifying the FileSystemInput, you must make sure Amazon FSx for Lusture is linked your Amazon S3 that have training/validation data.\n",
    "For more information, see [Amazon SageMaker - Configure Data Input Channel to Use Amazon FSx for Lustre](https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html#model-access-training-data-fsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074c247-18fb-432e-98df-fe67c1f07d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "# Specify file system id.\n",
    "# e.g.) file_system_id = \"fs-0123456789abcdef0\"\n",
    "file_system_id = \"<your_file_system_id>\"\n",
    "\n",
    "# Specify directory path associated with the file system. You need to provide normalized and absolute path here.\n",
    "# When you specify `directory_path`, make sure that you provide the Amazon FSx file system path starting with `MountName`.\n",
    "# e.g.) file_system_directory_path = \"/1234abcd/ns1/sagemaker/DEMO-linear-mnist/train\"\n",
    "file_system_directory_path = \"/<mount_name>/<file_system_path>/<your_training_data_s3_location>\"\n",
    "\n",
    "# Specify your file system type: \"FSxLustre\".\n",
    "file_system_type = \"FSxLustre\"\n",
    "file_system_access_mode = \"ro\"\n",
    "\n",
    "file_system_input = FileSystemInput(\n",
    "    file_system_id=file_system_id,\n",
    "    file_system_type=file_system_type,\n",
    "    directory_path=file_system_directory_path,\n",
    "    file_system_access_mode=file_system_access_mode,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe45de-8e66-4b1b-bfc3-a82c656c6432",
   "metadata": {},
   "source": [
    "## Training the linear model\n",
    "\n",
    "Once we have the file system provisioned and file system input ready for training, the next step is to actually train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc44ad-3c6b-48ff-9284-bcac3ace4831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Give Amazon SageMaker Training Jobs Access to FileSystem Resources in Your Amazon VPC.\n",
    "security_groups_ids = [ \"<your_security_groups_ids>\" ]\n",
    "subnets = [ \"<your_subnets>\" ] # Should be the same as the subnet used for Amazon FSx\n",
    "\n",
    "linear = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    subnets=subnets,\n",
    "    security_group_ids=security_groups_ids,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "linear.set_hyperparameters(feature_dim=784, predictor_type=\"binary_classifier\", mini_batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5382a5c9-2ca4-4826-850c-0d7ddbf26fa3",
   "metadata": {},
   "source": [
    "Towards the end of the job you should see model artifact generated and uploaded to `output_location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6412b97d-9233-4947-a28d-d183f4d859aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linear.fit({\"train\": file_system_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77fa59-c466-4dc9-aa42-12d94d37c1b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up hosting for the model\n",
    "\n",
    "Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint. This will allow out to make predictions (or inference) from the model dyanamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cfc4a2-f11d-4a48-94a3-5abd4036b640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_job_name = linear.latest_training_job.name\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e283ac-762f-412c-8ad5-76b82aa9978b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_model_location = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(f\"trained model location: {trained_model_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67a97a-b35d-429e-b527-18f09dc24fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "model = Model(\n",
    "    image_uri=container,\n",
    "    model_data=trained_model_location,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ebb4e-878b-441a-a21a-cc088357e263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365ea1be-3378-47ca-896a-746c1f9aad86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"endpoint name: {model.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2500c745-81de-413e-8807-8f1103afd234",
   "metadata": {},
   "source": [
    "## Validate the model for use\n",
    "\n",
    "Finally, we can now validate the model for use.\n",
    "Let's try getting a prediction for a single record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530e52f-998e-4183-9a4c-3b08642c5286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "predictor = Predictor(model.endpoint_name, serializer=CSVSerializer(), deserializer = JSONDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75567f0e-8508-4511-a3bd-eff178363913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = predictor.predict(train_set[0][30], initial_args={\"ContentType\": \"text/csv\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d5c44-6734-4840-a47d-0bc4fb904d3c",
   "metadata": {},
   "source": [
    "OK, a single prediction works. We see that for one record our endpoint returned some JSON which contains `predictions`, including the `score` and `predicted_label`. In this case, `score` will be a continuous value between \\[0, 1\\] representing the probability we think the digit is a `0` or not. `predicted_label` will take a value of either `0` or `1` where (somewhat counterintuitively) `1` denotes that we predict the image is a `0`, while `0` denotes that we are predicting the image is not of a `0`.\n",
    "\n",
    "Let's do a whole batch of images and evaluate our predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6089e871-93a4-41ad-a242-668d8bace46c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = []\n",
    "for array in np.array_split(test_set[0], 100):\n",
    "    result = predictor.predict(array)\n",
    "    predictions += [r[\"predicted_label\"] for r in result[\"predictions\"]]\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a137a7fd-22fc-4303-92a4-e6f01007c7cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.crosstab(\n",
    "    np.where(test_set[1] == 0, 1, 0), predictions, rownames=[\"actuals\"], colnames=[\"predictions\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a98cd-e5b7-4644-b6e2-726aadefd68f",
   "metadata": {},
   "source": [
    "## (Optional) Clean Up\n",
    "\n",
    "If you're ready to be done with this notebook, please run the delete_endpoint line in the cell below. This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746f91b-a38e-44a7-b05a-a5883a341550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ebc2ed-f515-4bd0-bddb-40eba03e20b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (main, Dec 23 2022, 09:40:27) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
